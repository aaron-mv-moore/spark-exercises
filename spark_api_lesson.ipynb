{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 101\n",
    "\n",
    "In this lesson we will cover the basics of working with spark dataframes, and\n",
    "show how spark dataframes are different from the pandas dataframes we have\n",
    "been working with.\n",
    "\n",
    "While spark dataframes might superficially look like pandas dataframes, and\n",
    "even share some of the same methods and syntax, it is important to keep in\n",
    "mind they are 2 seperate types of objects, and, while spark and pandas code\n",
    "might look superficially similar, it tends to be semantically very different.\n",
    "\n",
    "We'll begin by creating the spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/18 13:52:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create session\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://aarons-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10db28100>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframes\n",
    "\n",
    "Spark can convert any pandas dataframe into a spark dataframe with a simple\n",
    "method call. For this lesson, we will use this functionality to demonstrate\n",
    "the differences between spark and pandas dataframes and explore how to work\n",
    "with spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n group\n",
       "0  0     c\n",
       "1  1     c\n",
       "2  2     b\n",
       "3  3     a\n",
       "4  4     c"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1349)\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    dict(n=np.arange(20), group=np.random.choice(list('abc'), 20))\n",
    ")\n",
    "pandas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start with a simple pandas dataset, and now we will convert it to a\n",
    "spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[n: bigint, group: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, while we do see the column names, we don't see the data in the\n",
    "dataframe like we would with a pandas dataframe. This is because spark is\n",
    "*lazy*, in that it won't show us values until it has to. For the purposes of\n",
    "looking at the first few rows of our data, we can use the `.show` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  n|group|\n",
      "+---+-----+\n",
      "|  0|    c|\n",
      "|  1|    c|\n",
      "|  2|    b|\n",
      "|  3|    a|\n",
      "|  4|    c|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.head()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like pandas dataframes, spark dataframes have a .describe method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----+\n",
      "|summary|                n|group|\n",
      "+-------+-----------------+-----+\n",
      "|  count|               20|   20|\n",
      "|   mean|              9.5| null|\n",
      "| stddev|5.916079783099616| null|\n",
      "|    min|                0|    a|\n",
      "|    max|               19|    c|\n",
      "+-------+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which, also like pandas, returns another dataframe. However, since this is a\n",
    "spark dataframe, we have to explicitly show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "mpg = data('mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = spark.createDataFrame(mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default spark will show the first 20 rows, but we can specify how many we\n",
    "want by passing a number to `.show`.\n",
    "\n",
    "Let's use some different data so that we have a more robust dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------------+-----------------+------------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "|summary|manufacturer|            model|            displ|              year|              cyl|     trans|drv|               cty|              hwy|  fl|  class|\n",
      "+-------+------------+-----------------+-----------------+------------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "|  count|         234|              234|              234|               234|              234|       234|234|               234|              234| 234|    234|\n",
      "|   mean|        null|             null|3.471794871794872|            2003.5|5.888888888888889|      null|4.0|16.858974358974358|23.44017094017094|null|   null|\n",
      "| stddev|        null|             null|1.291959031083935|4.5096463133204585|1.611534484684289|      null|0.0| 4.255945678889394|5.954643441166448|null|   null|\n",
      "|    min|        audi|      4runner 4wd|              1.6|              1999|                4|  auto(av)|  4|                 9|               12|   c|2seater|\n",
      "|    max|  volkswagen|toyota tacoma 4wd|              7.0|              2008|                8|manual(m6)|  r|                35|               44|   r|    suv|\n",
      "+-------+------------+-----------------+-----------------+------------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mpg.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another difference from pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "|model|displ|cyl|\n",
      "+-----+-----+---+\n",
      "|   a4|  1.8|  4|\n",
      "|   a4|  1.8|  4|\n",
      "|   a4|  2.0|  4|\n",
      "|   a4|  2.0|  4|\n",
      "|   a4|  2.8|  6|\n",
      "+-----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking at a subset of columns:\n",
    "# pandas: df[[''model', 'displ', 'cyl']]\n",
    "mpg.select(mpg.model, mpg.displ, mpg.cyl).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "# \n",
    "mpg.model.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this expression would produce a Series of values from a pandas\n",
    "dataframe, for a spark dataframe this produces a Column object, which is an\n",
    "object that represents a vertical slice of a dataframe, but does not contain\n",
    "the data itself.\n",
    "\n",
    "One way to use our column objects is to use them in combination with the\n",
    "`.select` method. `.select` is very powerful, and lets us specify what data we\n",
    "want to see in the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hwy: bigint, cty: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.cty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, notice that we don't see any data, instead we see the new dataframe\n",
    "that is produced. To see the actual data, we'll again need to use `.show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|hwy|cty|\n",
      "+---+---+\n",
      "| 29| 18|\n",
      "| 29| 21|\n",
      "| 31| 20|\n",
      "| 30| 21|\n",
      "+---+---+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .select will give us the set of columns we want to see\n",
    "# we can reference the columns with dot notation just like pandas Series\n",
    "# and if we want to see the results of our action, we can chain a .show()\n",
    "# at the end of the command\n",
    "mpg.select(mpg.hwy, mpg.cty).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our column objects support a numer of operations, including the arithmetic\n",
    "operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(hwy + 1)'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.hwy + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get back a column that represents the values from the original `hwy`\n",
    "column with 1 added to them. To actually see this data, we'd need to select it\n",
    "and show the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|hwy|(hwy + 1)|\n",
      "+---+---------+\n",
      "| 29|       30|\n",
      "| 29|       30|\n",
      "| 31|       32|\n",
      "| 30|       31|\n",
      "| 26|       27|\n",
      "+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.hwy+1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a column object, we can use the `.alias` method to rename it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|highway|highway_plus_one|\n",
      "+-------+----------------+\n",
      "|     29|              30|\n",
      "|     29|              30|\n",
      "|     31|              32|\n",
      "|     30|              31|\n",
      "|     26|              27|\n",
      "+-------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mpg\n",
    "mpg.select(mpg.hwy.alias('highway'), (mpg.hwy+1).alias('highway_plus_one')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also store column objects in variables and reference them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# col1: the hwy column from the mpg dataframe, aliased as highway_mileage\n",
    "# col2: the calculation of the highway column divided by two, aliased as highway_mileage_halved\n",
    "col1 = mpg.hwy.alias('highway_mileage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'hwy AS highway_mileage'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|highway_mileage|\n",
      "+---------------+\n",
      "|             29|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(col1).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to create columns\n",
    "\n",
    "In addition to the syntax we've seen above, we can create columns with the\n",
    "`col` and `expr` functions from `pyspark.sql.functions` module.\n",
    "\n",
    "### `col`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'hwy'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mpg.select(mpg.hwy).show(5)\n",
    "F.col('hwy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|hwy|\n",
      "+---+\n",
      "| 29|\n",
      "| 29|\n",
      "+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select('hwy').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_column = (2 / ((1/ F.col('hwy')) + (1/F.col('cty')))).alias('avg_mileage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(2 / ((1 / hwy) + (1 / cty))) AS avg_mileage'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      avg_mileage|\n",
      "+-----------------+\n",
      "|22.21276595744681|\n",
      "|            24.36|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(avg_column).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mix and match the syntax we use, and the column object produced by the\n",
    "`col` function is the same as the the previous column object we saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpg,\n",
    "# select the column hwy, alias it\n",
    "# select the column cty, alias it,\n",
    "# select the column avg_column, alias it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a variable named `avg_column` that represents the average of the\n",
    "highway and city mileage of each vehicle. This variable is created by using\n",
    "the `col` function to produce pyspark Column objects and using the arithmetic\n",
    "operators to combine them.\n",
    "\n",
    "Next we select the original highway and city mileage columns, in addition to\n",
    "our new average mileage column. We demonstrate the `col` function to select\n",
    "the `hwy` column and refer to the city mileage column with the `df.cty`\n",
    "syntax we saw previously. We also give all of our columns more readable\n",
    "aliases before showing the resulting dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `expr`\n",
    "\n",
    "The `expr` function is more powerful than `col`. It does everything `col` does\n",
    "and more. `expr` returns the same type of column object, but allows us to\n",
    "express manipulations to the column within the string that defines the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_col = F.expr('hwy + 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|(hwy + 1)|\n",
      "+---------+\n",
      "|       30|\n",
      "|       30|\n",
      "+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(my_col).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(\n",
    "    # the same as `col`\n",
    "    # an arithmetic expression\n",
    "    # using an alias\n",
    "    # a combination of the above\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the columns created below are identical, and which syntax to use\n",
    "is merely a style choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "As we've seen through the column definitions, spark is very flexible and\n",
    "allows us many different ways to express ourselves. Another way that is fairly\n",
    "different than what we've seen above is through **spark SQL**, which lets us\n",
    "write SQL queries against our spark dataframes.\n",
    "\n",
    "In order to start using spark SQL, we'll first \"register\" the table with\n",
    "spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "mpg.createOrReplaceTempView('mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|cty|\n",
      "+---+\n",
      "| 18|\n",
      "| 21|\n",
      "| 20|\n",
      "| 21|\n",
      "| 16|\n",
      "| 18|\n",
      "| 18|\n",
      "| 18|\n",
      "| 16|\n",
      "| 20|\n",
      "| 19|\n",
      "| 15|\n",
      "| 17|\n",
      "| 17|\n",
      "| 15|\n",
      "| 15|\n",
      "| 17|\n",
      "| 16|\n",
      "| 14|\n",
      "| 11|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "spark.sql('''\n",
    "SELECT cty FROM mpg\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a sql query against the `mpg` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manufacturer', 'string'),\n",
       " ('model', 'string'),\n",
       " ('displ', 'double'),\n",
       " ('year', 'bigint'),\n",
       " ('cyl', 'bigint'),\n",
       " ('trans', 'string'),\n",
       " ('drv', 'string'),\n",
       " ('cty', 'bigint'),\n",
       " ('hwy', 'bigint'),\n",
       " ('fl', 'string'),\n",
       " ('class', 'string')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resulting value is another dataframe. As we know, in order to\n",
    "view the values in a dataframe, we need to use `.show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that all of the methods for creating / manipulating\n",
    "dataframes outlined above are the same in terms of performance as well. All of\n",
    "the resulting dataframes get turned into the same spark code that gets\n",
    "executed on the JVM, so it really is just a style choice as to which to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Casting\n",
    "\n",
    "We can view the types of the column in our dataframe in one of two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas: .astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hwy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .printSchema() is similar to a .info() call\n",
    "mpg.select(\n",
    "mpg.hwy.cast('string')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both provide the same information.\n",
    "\n",
    "To convert from one type to another, we can use the `.cast` method on a\n",
    "column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|model|\n",
      "+-----+\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "mpg.model.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if a value is not able to be converted, it will be replaced with\n",
    "null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Built-in Functions\n",
    "\n",
    "We've used the `col` and `expr` functions, but there are many other functions\n",
    "within the `pyspark.sql.functions` module, all of which operate on pyspark\n",
    "dataframe columns. Here we'll demonstrate several:\n",
    "\n",
    "- `concat`: to concatenate strings\n",
    "- `sum`: to sum a group\n",
    "- `avg`: to take the average of a group\n",
    "- `min`: to find the minimum\n",
    "- `max`: to find the maximum\n",
    "\n",
    "**_Note that importing the `sum` function directly will override the built-in\n",
    "`sum` function._** This means you will get an error if you try to sum a list\n",
    "of numbers, because `sum` will refernce the pyspark `sum` function, which\n",
    "works with pyspark dataframe columns, while the built-in `sum` function works\n",
    "with lists of numbers. The same holds true for the built in `min` and `max`\n",
    "functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The pyspark avg and mean functions are aliases of eachother\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!tip \"`pyspark` imports\"\n",
    "    In this lesson we will explicitly import any functions from `pyspark.sql.functions` that we use, but it very common to see something like:\n",
    "    \n",
    "    ```\n",
    "    from pyspark.sql.functions import *\n",
    "    ```\n",
    "    \n",
    "    which will import *all* of the functions from the `pyspark.sql.functions` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can chain an alias on a concat function call\n",
    "# we need to cast cylinders with a lit() function call to cast it as a string literal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use a string literal as part of our select, we'll need to use the\n",
    "`lit` function, otherwise spark will try to resolve our string as a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select the concatenation of the number of cylinders (the value from\n",
    "the `cyl` column) and the string literal \" cylinders\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More `pyspark` Functions for String Manipulation\n",
    "\n",
    "Let's take a look at a couple more functions for string manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import regexp_extract, regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to demonstrate these functions we'll create a dataframe with some text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"address\": [\n",
    "                \"600 Navarro St ste 600, San Antonio, TX 78205\",\n",
    "                \"3130 Broadway St, San Antonio, TX 78209\",\n",
    "                \"303 Pearl Pkwy, San Antonio, TX 78215\",\n",
    "                \"1255 SW Loop 410, San Antonio, TX 78227\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "textdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `regexp_extract` function lets us specify a regular expression with at least one capture group, and create a new column based on the contents of a capture group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in my select call:\n",
    "# address, the column as it stands\n",
    "# regexp_extract, acting as a re.extract, aliasing that with a .alias method \n",
    "textdf.select(\n",
    "    \"address\",\n",
    "    regexp_extract(####).alias(\"street_no\"),\n",
    "    regexp_extract(####).alias(\"street\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the first argument to `regexp_extract` is the name of the string column to extract from, the second argument is the regular expression itself, and the last argument specifies which capture group we want to use. If, for example, our regular expression had 2 capture groups in it and we wanted the contents of the 2nd group, we would specify a 2 here.\n",
    "\n",
    "In addition to `regexp_extract`, `regexp_replace` lets us make substitutions based on a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf.select(\n",
    "    \"address\",\n",
    "    #regexp_replace('subject', 'pattern', 'replacement)\n",
    "    regexp_replace(####).alias(\"city_state_zip\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example above, we obtain just the city, state, and zip code of the address by replacing everything up to the first comma with an empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.filter` and `.where`\n",
    "\n",
    "Spark provides two dataframe methods, `.filter` and `.where`, which both allow\n",
    "us to select a subset of the rows of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas:\n",
    "# mpg[mpg.cyl == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When and Otherwise\n",
    "\n",
    "Similar to an `IF` in Excel, `CASE...WHEN` in SQL, or `np.where` in python,\n",
    "spark provides a `when` function.\n",
    "\n",
    "The `when` function lets us specify a condition, and a value to produce if\n",
    "that condition is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that if the condition we specified is false, `null` will be\n",
    "produced. Instead of null, we can specify a value to use if our condition is\n",
    "false with the `.otherwise` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify multiple conditions, we can chain `.when` calls. The first\n",
    "condition that is met will be the value that is used, and if none of the\n",
    "conditions are met the value specified in the `.otherwise` will be used (or\n",
    "`null` if you don't provide a `.otherwise`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that a car with a `displ` of 1.8 matches both conditions we\n",
    "specified, but `small` is produced because it is associated with the first\n",
    "matching condition. For any value between 2 and 3, `medium` will be produced,\n",
    "and anything larger than 3 will produce `large`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and Ordering\n",
    "\n",
    "Spark lets us sort the rows in our dataframe by one or multiple columns with\n",
    "two methods: `.sort`, and `.orderBy`. `.sort` and `.orderBy` are aliases of\n",
    "each other and do the exact same thing. Like other methods we've seen, `.sort`\n",
    "takes in a Column object or a string that is the name of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, values are sorted in ascending order. To sort in descending order,\n",
    "we can use the `.desc` method on any Column object, or the `desc` function\n",
    "from `pyspark.sql.functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify sorting by multiple columns, we provide each column as a separate\n",
    "argument to `.sort`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will first reverse alphabetically by number of cylinders from lowest to highest, then by the vehicle's highway\n",
    "mileage, from greatest to smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Aggregating\n",
    "\n",
    "To aggregate our data by group, we can use the `.groupBy` method. Like with\n",
    "`.select`, we can pass either Column objects or strings that are column names\n",
    "to `.groupBy`. All of the expressions below are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.groupBy(mpg.cyl)\n",
    "mpg.groupBy(col('cyl'))\n",
    "mpg.groupBy('cyl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is grouped, we need to specify an aggregation. We can use one of\n",
    "the aggregate functions we imported earlier, alond with a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To group by multiple columns, pass each of the columns a a separate argument\n",
    "to `.groupBy` (Note that this is different from pandas, where we would need to\n",
    "pass a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `.groupBy`, we can use `.rollup`, which will do the same\n",
    "aggregations, but will also include the overall total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the null value in `cyl` indicates the total count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the example above, the null row represents the overall average highway\n",
    "mileage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosstabs and Pivot Tables\n",
    "\n",
    "In addition to groupby, spark provides a couple other ways to do aggregation.\n",
    "One of which is `.crosstab`. This is very similary to pandas `.crosstab`\n",
    "function, in that it calculates the number of occurances of each unique value\n",
    "from the two passed columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.crosstab` simply does counts, if we want a different aggregation, we can use\n",
    "`.pivot`. For example, to find the average highway mileage for each\n",
    "combination of car class and number of cylinders, we could write the\n",
    "following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the unique values from the column we group by will be the rows in the\n",
    "resulting dataframe, and the unique values from the column we pivot on will\n",
    "become the columns. The values in each cell will be equal to the aggregation\n",
    "we specified over the group of values defined by the intersection of the rows\n",
    "and the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Let's take a look at how spark handles missing data. First we'll create a dataframe that has a few missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\"x\": [1, 2, np.nan, 4, 5, np.nan], \"y\": [np.nan, 0, 0, 3, 1, np.nan]}\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides two main ways to deal with missing values:\n",
    "\n",
    "- `.fill`: to replace missing values with a specified value\n",
    "- `.drop`: to drop rows containing missing values\n",
    "\n",
    "Both methods are accessed through the `.na` property. We'll look at some examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both methods, we can specify that we only want to fill or drop values in a specific column with a second argument:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that above the na values in the `x` column were filled with 0, but the na values in y were left alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the rows that had an na value for the y column were dropped, but the rows with na values for only the x column are still present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Dataframe Manipulation Examples\n",
    "\n",
    "Let's take a look at some more examples of working with spark dataframes. For\n",
    "these examples, we'll be working with a dataset of observations of the\n",
    "weather in seattle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(weather)\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the number of rows and columns in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first find the dates where the data starts and stops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date, max_date = ####\n",
    "min_date, max_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `.select` to select the minimum date and the maximum date.\n",
    "`.first` returns us the first row of our results, which consists of two value,\n",
    "and so can be unpacked into the `min_date` and `max_date` variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will combine the temp max and min columns into a single column,\n",
    "`temp_avg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn is similar to our df.assign()\n",
    "weather = ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the total amount of rainfall for each month. We'll do\n",
    "this by first creating a month column, then grouping by the month, and\n",
    "finally, aggregating by taking the sum of the precipitation. To do this we will need to use the `month` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import month, year, quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.sort` at the end isn't necessary, but presents that data in a friendlier\n",
    "way.\n",
    "\n",
    "Let's now take a look at the average temperature for each type of weather in\n",
    "December 2013:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month will rip out the month numeral from the date\n",
    "# establish a filter df[df.month == 12]\n",
    "# pass a second filter of df[df.year == 2013]\n",
    "# aggregate based on weather tag, present average temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first have a couple of `.filter` calls in order to restrict our data\n",
    "to December of 2013. We then group by the weather column, and lastly,\n",
    "aggregate by taking the average of our `temp_avg` column. The combination of\n",
    "group by and agg will calculate the average temperature for each unique value\n",
    "of the `weather` column.\n",
    "\n",
    "Let's now find out how many days had freezing temperatures in each month of\n",
    "2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last example, let's calculate the average temperature for each quarter of\n",
    "each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the `quarter` and `year` columns, then group by these two new columns, and take the average temperature as our aggregate. Lastly, we sort by the year and quarter for presentation purposes.\n",
    "\n",
    "We could also use a pivot table like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here instead of grouping by two columns, we grouped by the first column and pivoted on the other column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "Like pandas and sql, spark has functionality that lets us combine two tabular\n",
    "datasets, known as a **join**.\n",
    "\n",
    "We'll start by creating some data that we can join together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4, 5, 6],\n",
    "            \"name\": [\"bob\", \"joe\", \"sally\", \"adam\", \"jane\", \"mike\"],\n",
    "            \"role_id\": [1, 2, 3, 3, np.nan, np.nan],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "roles = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4],\n",
    "            \"name\": [\"admin\", \"author\", \"reviewer\", \"commenter\"],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(\"--- users ---\")\n",
    "users.show()\n",
    "print(\"--- roles ---\")\n",
    "roles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join two dataframes together, we'll need to call the `.join` method on one\n",
    "of them and supply the other as an argument. In addition, we'll need to supply\n",
    "the condition on which we are joining. In our case, we are joining where the\n",
    "`role_id` column on the users table is equal to the `id` column on the roles\n",
    "table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, spark will perform an inner join, meaning that records from both\n",
    "dataframes will have a match with the other. We can also specify either a left\n",
    "or a right join, which will keep all of the records from either the left or\n",
    "right side, even if those records don't have a match with the other dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that examples above have a duplicate `id` column. There are several\n",
    "ways we could go about dealing with this:\n",
    "\n",
    "- alias each dataframe + explicitly select columns after joining (this could also be implemented with spark SQL)\n",
    "- rename duplicated columns before merging\n",
    "- drop duplicated columns after the merge (`.drop(right.id)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (or Lack Therof)\n",
    "\n",
    "Spark does not provide a way to do visualization with their dataframes. To\n",
    "visualize data from spark, you should use the `.toPandas` method on a spark\n",
    "dataframe to convert it to a pandas dataframe, then visualize as you normally\n",
    "would.\n",
    "\n",
    "!!!warning \"Converting to A Pandas Dataframe\"\n",
    "    Converting a spark dataframe to a pandas dataframe will pull all the data into memory, so make sure you have enough available memory to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Using the [repo setup directions](https://ds.codeup.com/fundamentals/git/), setup a new local and remote repository named `spark-exercises`. The local version of your repo should live inside of `~/codeup-data-science`. This repo should be named `spark-exercises`\n",
    "\n",
    "Save this work in your `spark-exercises` repo. Then add, commit, and push your changes.\n",
    "\n",
    "Create a jupyter notebook or python script named `spark101` for this exercise.\n",
    "\n",
    "1. Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "    - The name of the column should be `language`\n",
    "    - View the schema of the dataframe\n",
    "    - Output the shape of the dataframe\n",
    "    - Show the first 5 records in the dataframe\n",
    "\n",
    "1. Load the `mpg` dataset as a spark dataframe.\n",
    "\n",
    "    1. Create 1 column of output that contains a message like the one below:\n",
    "\n",
    "            The 1999 audi a4 has a 4 cylinder engine.\n",
    "\n",
    "        For each vehicle.\n",
    "\n",
    "    1. Transform the `trans` column so that it only contains either `manual` or `auto`.\n",
    "\n",
    "1. Load the `tips` dataset as a spark dataframe.\n",
    "\n",
    "    1. What percentage of observations are smokers?\n",
    "    1. Create a column that contains the tip percentage\n",
    "    1. Calculate the average tip percentage for each combination of sex and smoker.\n",
    "\n",
    "1. Use the seattle weather dataset referenced in the lesson to answer the questions below.\n",
    "\n",
    "    - Convert the temperatures to fahrenheit.\n",
    "    - Which month has the most rain, on average?\n",
    "    - Which year was the windiest?\n",
    "    - What is the most frequent type of weather in January?\n",
    "    - What is the average high and low temperature on sunny days in July in 2013 and 2014?\n",
    "    - What percentage of days were rainy in q3 of 2015?\n",
    "    - For each year, find what percentage of days it rained (had non-zero precipitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
